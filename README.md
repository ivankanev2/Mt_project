# EN‚ÜíBG Machine Translation Evaluation

This project evaluates **English ‚Üí Bulgarian translation models** (both MT and LLMs) against professional reference translations.  
It handles noisy DOCX/PDF/XML documents, aligns English source lines with Bulgarian references, and computes quality metrics (BLEU, chrF).  
Outputs are saved in both **CSV** and **Excel** for easy inspection.

---

## Features
- üìÑ Extracts text from `.docx`, `.pdf`, `.xml` (XLIFF/TMX) references
- üìä Aligns EN and BG documents with a **global DP aligner** (one row per English line)
- üîç Outputs **compare tables**, **disagreements only**, and a **summary sheet**
- ‚úÖ Supports glossary enforcement and BG normalization (optional)
- üìà Computes **BLEU** and **chrF** metrics using [sacreBLEU](https://github.com/mjpost/sacrebleu)
- üî§ Writes both `.csv` (UTF-8 BOM) and `.xlsx` (Excel-ready) files

---

## Installation

Clone this repo and set up a virtual environment:

```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

python -m venv .venv
source .venv/bin/activate   # on Windows: .venv\Scripts\activate
pip install -r requirements.txt
```
Usage

Run evaluation on your English source and Bulgarian reference:
python batch_eval.py \
  --src data/english_ref.docx \
  --ref data/bulgarian_ref.docx \
  --models "Helsinki-NLP/opus-mt-en-bg" \
  --outdir results_run --clean_outdir \
  --no_glossary --no_bg_normalize \
  --thr 0.50 \
  --export_disagreements

  Arguments
  
	‚Ä¢	--src : English source file (.docx / .pdf)
  
	‚Ä¢	--ref : Bulgarian reference file (.docx / .pdf / .xml)
  
	‚Ä¢	--models : Comma-separated Hugging Face model IDs
  
	‚Ä¢	--outdir : Results folder
  
	‚Ä¢	--clean_outdir : Clear old results before writing new ones
  
	‚Ä¢	--no_glossary : Disable glossary term enforcement
  
	‚Ä¢	--no_bg_normalize : Disable Bulgarian variant normalization
  
	‚Ä¢	--thr : Similarity threshold for DP alignment (default 0.50)
  
	‚Ä¢	--export_disagreements : Write a file with mismatched rows only

Outputs

Each run produces:

	‚Ä¢	system_<model>.out
  
‚Üí Model translations (one per English line)

	‚Ä¢	compare_anchored_<model>.csv / .xlsx
  
‚Üí Side-by-side table:

English | System BG | Reference BG | en_idx | bg_idx | segment

	‚Ä¢	disagreements_<model>.csv / .xlsx
  
‚Üí Only rows where system ‚â† reference

	‚Ä¢	summary.csv / .xlsx
  
‚Üí Overall results per model (aligned pairs, BLEU, chrF, file paths)

	‚Ä¢	debug/dbg_*
  
‚Üí Debug dumps of raw and cleaned extraction

Example

Console output:
DP input sizes: EN=3046 | BG_REF=3161

Translated 3046 lines.

DP produced rows: 3046 (should be close to number of EN lines)

aligned subset: n=2895, BLEU=43.83, chrF=68.53

Important things to note:

	‚Ä¢	BLEU/chrF are computed case-insensitive (all lowercased before scoring).
  
	‚Ä¢	Segmentation is line-based (paragraphs/table cells), not sentence-segmented.
  
	‚Ä¢	Empty BG cells in the compare table mean no confident match (gap).
  
	‚Ä¢	Use --glossary (CSV file) to enforce domain-specific terms if needed.
  
	‚Ä¢	For large docs, evaluation may take several minutes.
